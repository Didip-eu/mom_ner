# Documentation for Monasterium Spacy NER Model

* The folder [data](https://github.com/acdh-oeaw/mon_ner/tree/experiments_varvara_arzt/data) contains a [.jsonl file](https://github.com/acdh-oeaw/mon_ner/blob/experiments_varvara_arzt/data/annotations/monner1.jsonl) with the annotated data that were used to train a spacy NER model.
* [prepare_data.py](https://github.com/acdh-oeaw/mon_ner/blob/experiments_varvara_arzt/src/prepare_data.py) converts the .jsonl file to the binary .spacy format, so that the data can be used to train a NER model. Finally the data are shuffled and split into training and test set (trainset = 900, testset = 100).
* The folder [models](https://github.com/acdh-oeaw/mon_ner/tree/experiments_varvara_arzt/models) contains a spacy tok2vec NER model finetuned on the annotated monasterium data. More details concerning training a spacy model see on [spacy page](https://spacy.io/usage/training). The following [jupyter notebook](https://github.com/acdh-oeaw/mon_ner/blob/experiments_varvara_arzt/src/usage_example_ner_model.ipynb) shows how to load this spacy model and use it for named entity recognition.
* Training a spacy transformer model results in better performance (f1-score = 0.84, precision = 0.83, recall = 0.85) than a spacy tok2vec model with default configuration (f1-score = 0.75, precision = 0.76, recall = 0.73). In order to train a spacy transformer model you need to choose a GPU (transformer) for hardware while creating a config file on the [spacy website](https://spacy.io/usage/training).